#!/usr/bin/env ruby

require 'rubygems'
require 'httpclient'
require 'json'

class Crawl
  
  def initialize(http, links, url, base)
    @http = http
    @links = links
    @url = url.gsub(/&amp;/, '&')
    @base = base
  end
  
  def crawl()
    return if /^mailto/.match(@url)
    url = @url.match(/http(s)?:\/\//) ? @url : "#{@base}#{@url}"
    return if @links.has_key?(url)
    
    if /^#{@base}/.match(url)
      page = @http.get(url)
      @links[url] = case page.status.to_s.to_i
      when (0..299)
        :ok
      when (300..399)
        :redirect
      when (400..499)
        :client_error
      when (500..599)
        :server_error
      end
      page.body.scan(/href="(.*?)"/i).flatten.each do |link|
        Crawl.new(@http, @links, link, @base).crawl
      end
    else
      begin
        page = @http.get(url)
        @links[url] = case page.status.to_s.to_i
        when (0..299)
          :external_ok
        when (300..399)
          :external_redirect
        when (400..499)
          :external_client_error
        when (500..599)
          :external_server_error
        end
      rescue
        @links[url] = :external_connection_error
      end
    end
    puts "#{@links[url]}: #{url}"
  end
end

links = {}

http = HTTPClient.new
http.ssl_config.verify_mode = OpenSSL::SSL::VERIFY_NONE

Crawl.new(http, links, 'http://localhost:8080', 'http://localhost:8080').crawl
open('crawl.json', 'w') {|f| f.write links.to_json }
